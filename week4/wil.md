PaperBench - 최첨단 AI 연구 복제를 위한 벤치마크
PaperBench는 인공지능 에이전트의 최신 AI 연구 복제 능력을 평가하기 위해 고안된 새로운 벤치마크이다. 이 벤치마크는 ICML 2024의 Spotlight 및 Oral 채택 논문 20편을 기준으로 하며, 에이전트는 해당 논문들을 처음부터 재현해야 한다.
복제 과제:  1.논문의 핵심 기여 이해 2.코드베이스 개발 3.실험 수행 및 재현 이 포함

PaperBench의 주요 목표 1. 대규모 언어 모델(LLM)의 AI 엔지니어링 능력을 정량적으로 측정 2.논문 수준의 작업을 실제로 수행할 수 있는지를 평가 3.인간 전문가와 AI의 능력 격차를 분석

구성 요소 (Benchmark Composition)
- 논문 수: 20편
- 세부 평가 태스크 수: 8,316개
- 평가 루브릭: 각 논문 복제 작업을 계층적으로 나눈 루브릭 개발, 루브릭은 논문 저자와 공동으로 설계, 각 루브릭은 명확한 채점 기준과 하위 태스크로 구성

자동 채점 시스템 : 효율적인 평가를 위해 PaperBench는 LLM 기반 자동 채점기를 개발
- 기능: 복제 시도 결과를 루브릭 기준으로 자동 채점
- 성능 평가: judge의 정확성을 평가하기 위한 별도의 Judge Benchmark 도입

모델 성능 평가 
- 평가 대상: Claude3.5 Sonnet - 오픈소스 기반 scaffolding 코드 사용
-> 결과: 평균 복제 점수: 21.0% : 전체 과제 대비 여전히 낮은 재현율 → 향후 개선 여지 확인

인간 기준선 
- Top-tier 머신러닝 박사과정생들을 대상으로 일부 태스크 수행 실험
-> 결과: AI 에이전트는 아직 인간 기준선을 능가하지 못함

오픈소스 공개 
- 전체 PaperBench 코드 및 루브릭, judge 시스템은 오픈소스로 공개
- 목적: AI 에이전트의 실질적 엔지니어링 역량 평가 연구 촉진